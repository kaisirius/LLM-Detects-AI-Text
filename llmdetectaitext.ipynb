{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport gc\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\n\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nfrom transformers import PreTrainedTokenizerFast\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-14T18:15:09.650556Z","iopub.execute_input":"2024-01-14T18:15:09.651003Z","iopub.status.idle":"2024-01-14T18:15:09.661269Z","shell.execute_reply.started":"2024-01-14T18:15:09.650972Z","shell.execute_reply":"2024-01-14T18:15:09.659700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Importing data and reading them seperately for test and train**","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\nsub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\norg_train = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\n\ntrain = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:15:09.664105Z","iopub.execute_input":"2024-01-14T18:15:09.665033Z","iopub.status.idle":"2024-01-14T18:15:11.120067Z","shell.execute_reply.started":"2024-01-14T18:15:09.664986Z","shell.execute_reply":"2024-01-14T18:15:11.118752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting the external train dataset as pie chart**","metadata":{}},{"cell_type":"code","source":"value_counts = train['RDizzl3_seven'].value_counts()\ncolors=['blue','orange']\nplt.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%', startangle=90,colors=colors)\nplt.title('Percentage that is AI generated in External Train Dataset')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:15:11.122488Z","iopub.execute_input":"2024-01-14T18:15:11.123079Z","iopub.status.idle":"2024-01-14T18:15:11.292763Z","shell.execute_reply.started":"2024-01-14T18:15:11.123030Z","shell.execute_reply":"2024-01-14T18:15:11.291070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The provided data set is skewed**","metadata":{}},{"cell_type":"code","source":"percentage_zeros=(org_train['generated'] == 0).mean() * 100\npercentage_ones=(org_train['generated'] == 1).mean() * 100\n\ncolors=['blue','orange']\nbars=plt.bar(['Not Generated', 'Generated'], [percentage_zeros, percentage_ones],color=colors)\n\nplt.xlabel('Values')\nplt.ylabel('Percentage')\nplt.title('Percentage of Generated and Not Generated in given train dataset')\n\nfor bar, percentage in zip(bars, [percentage_zeros, percentage_ones]):\n    plt.text(bar.get_x() + bar.get_width() / 2 - 0.15, bar.get_height() + 1,\n             f'{percentage:.2f}%', ha='center', va='bottom')\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:15:11.295095Z","iopub.execute_input":"2024-01-14T18:15:11.296017Z","iopub.status.idle":"2024-01-14T18:15:11.588853Z","shell.execute_reply.started":"2024-01-14T18:15:11.295951Z","shell.execute_reply":"2024-01-14T18:15:11.587460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This is how our train dataset looks**","metadata":{}},{"cell_type":"code","source":"print(train.tail())","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:15:11.592580Z","iopub.execute_input":"2024-01-14T18:15:11.593044Z","iopub.status.idle":"2024-01-14T18:15:11.602979Z","shell.execute_reply.started":"2024-01-14T18:15:11.592997Z","shell.execute_reply":"2024-01-14T18:15:11.601544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Removing the duplicates from row in text column**","metadata":{}},{"cell_type":"code","source":"train = train.drop_duplicates(subset=['text'])\n\ntrain.reset_index(drop=True, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:15:11.604639Z","iopub.execute_input":"2024-01-14T18:15:11.605641Z","iopub.status.idle":"2024-01-14T18:15:11.695088Z","shell.execute_reply.started":"2024-01-14T18:15:11.605603Z","shell.execute_reply":"2024-01-14T18:15:11.693827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.text.values","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:15:11.696843Z","iopub.execute_input":"2024-01-14T18:15:11.697328Z","iopub.status.idle":"2024-01-14T18:15:11.708067Z","shell.execute_reply.started":"2024-01-14T18:15:11.697285Z","shell.execute_reply":"2024-01-14T18:15:11.706705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LOWERCASE = False\nVOCAB_SIZE = 30522","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:15:11.709630Z","iopub.execute_input":"2024-01-14T18:15:11.710262Z","iopub.status.idle":"2024-01-14T18:15:11.718465Z","shell.execute_reply.started":"2024-01-14T18:15:11.710228Z","shell.execute_reply":"2024-01-14T18:15:11.717482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Normalizing and Tokenizing our dataset using Byte Pair Encoding technique**","metadata":{}},{"cell_type":"code","source":"raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n\nraw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\nraw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\ntrainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n\ndataset = Dataset.from_pandas(test[['text']])\n\ndef train_corp_iter():\n    \"\"\"\n    A generator function for iterating over a dataset in chunks.\n    \"\"\"    \n    for i in range(0, len(dataset), 1000):\n        yield dataset[i : i + 1000][\"text\"]\n\nraw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\n\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=raw_tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\",\n)\n\ntokenized_texts_test = []\n\nfor text in tqdm(test['text'].tolist()):\n    tokenized_texts_test.append(tokenizer.tokenize(text))\n\ntokenized_texts_train = []\n\nfor text in tqdm(train['text'].tolist()):\n    tokenized_texts_train.append(tokenizer.tokenize(text))","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:15:11.720062Z","iopub.execute_input":"2024-01-14T18:15:11.720400Z","iopub.status.idle":"2024-01-14T18:18:11.465224Z","shell.execute_reply.started":"2024-01-14T18:15:11.720372Z","shell.execute_reply":"2024-01-14T18:18:11.463448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_texts_test[1]","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:18:11.467103Z","iopub.execute_input":"2024-01-14T18:18:11.467650Z","iopub.status.idle":"2024-01-14T18:18:11.477634Z","shell.execute_reply.started":"2024-01-14T18:18:11.467601Z","shell.execute_reply":"2024-01-14T18:18:11.476302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Initializing dummy funtion before vectorization**","metadata":{}},{"cell_type":"code","source":"def dummy(text):\n    \"\"\"\n    A dummy function to use as tokenizer for TfidfVectorizer. It returns the text as it is since we already tokenized it.\n    \"\"\"\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:18:11.479726Z","iopub.execute_input":"2024-01-14T18:18:11.480580Z","iopub.status.idle":"2024-01-14T18:18:11.491764Z","shell.execute_reply.started":"2024-01-14T18:18:11.480542Z","shell.execute_reply":"2024-01-14T18:18:11.490276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Using TF-IDFVectorizer tool from scikit learn and using N-gram technique for feature extraction**","metadata":{}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, analyzer = 'word',\n    tokenizer = dummy,\n    preprocessor = dummy,\n    token_pattern = None, strip_accents='unicode'\n                            )\n\nvectorizer.fit(tokenized_texts_test)\n\nvocab = vectorizer.vocabulary_\n\nprint(vocab)\n\nvectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, vocabulary=vocab,\n                            analyzer = 'word',\n                            tokenizer = dummy,\n                            preprocessor = dummy,\n                            token_pattern = None, strip_accents='unicode'\n                            )\n\ntf_train = vectorizer.fit_transform(tokenized_texts_train)\ntf_test = vectorizer.transform(tokenized_texts_test)\n\ndel vectorizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:18:11.493896Z","iopub.execute_input":"2024-01-14T18:18:11.494327Z","iopub.status.idle":"2024-01-14T18:23:51.631592Z","shell.execute_reply.started":"2024-01-14T18:18:11.494292Z","shell.execute_reply":"2024-01-14T18:23:51.630274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"{'ĠAaa Ġbbb Ġccc': 0, 'Ġbbb Ġccc .': 6, 'ĠAaa Ġbbb Ġccc .': 1, 'ĠBbb Ġccc Ġddd': 2, 'Ġccc Ġddd .': 7, 'ĠBbb Ġccc Ġddd .': 3, 'ĠCCC Ġddd Ġeee': 4, 'Ġddd Ġeee .': 8, 'ĠCCC Ġddd Ġeee .': 5}","metadata":{}},{"cell_type":"markdown","source":"**Extracting the values from label column in train dataset and storing in y_train**","metadata":{}},{"cell_type":"code","source":"y_train = train['label'].values","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:23:51.633821Z","iopub.execute_input":"2024-01-14T18:23:51.634251Z","iopub.status.idle":"2024-01-14T18:23:51.640684Z","shell.execute_reply.started":"2024-01-14T18:23:51.634216Z","shell.execute_reply":"2024-01-14T18:23:51.639669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_train","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:23:51.647099Z","iopub.execute_input":"2024-01-14T18:23:51.647593Z","iopub.status.idle":"2024-01-14T18:23:51.659621Z","shell.execute_reply.started":"2024-01-14T18:23:51.647555Z","shell.execute_reply":"2024-01-14T18:23:51.658185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Showing dimension of train dataset in form of matrix**","metadata":{}},{"cell_type":"code","source":"tf_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:23:51.661908Z","iopub.execute_input":"2024-01-14T18:23:51.662385Z","iopub.status.idle":"2024-01-14T18:23:51.674671Z","shell.execute_reply.started":"2024-01-14T18:23:51.662350Z","shell.execute_reply":"2024-01-14T18:23:51.673274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Showing dimension of test dataset in form of matrix**","metadata":{}},{"cell_type":"code","source":"tf_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:23:51.676178Z","iopub.execute_input":"2024-01-14T18:23:51.676548Z","iopub.status.idle":"2024-01-14T18:23:51.687587Z","shell.execute_reply.started":"2024-01-14T18:23:51.676518Z","shell.execute_reply":"2024-01-14T18:23:51.686017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ensembling the models on the basis of their weights and hence making the final prediction**","metadata":{}},{"cell_type":"code","source":"bayes_model = MultinomialNB(alpha=0.02)\nsgd_model = SGDClassifier(max_iter=10000, tol=1e-4, loss=\"modified_huber\")\n\n\nensemble = VotingClassifier(estimators=[('sgd', sgd_model), \n                                        ('nb', bayes_model)],\n                            weights=[0.91, 0.09], voting='soft', n_jobs=-1)\nensemble.fit(tf_train, y_train)\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:23:51.689980Z","iopub.execute_input":"2024-01-14T18:23:51.690840Z","iopub.status.idle":"2024-01-14T18:23:55.650237Z","shell.execute_reply.started":"2024-01-14T18:23:51.690790Z","shell.execute_reply":"2024-01-14T18:23:55.648552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Final prediction on test data set**","metadata":{}},{"cell_type":"code","source":"final_preds = ensemble.predict_proba(tf_test)[:,1]","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:23:55.653609Z","iopub.execute_input":"2024-01-14T18:23:55.654258Z","iopub.status.idle":"2024-01-14T18:23:55.664102Z","shell.execute_reply.started":"2024-01-14T18:23:55.654173Z","shell.execute_reply":"2024-01-14T18:23:55.662329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['generated'] = final_preds\nsub.to_csv('submission.csv', index=False)\nsub","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:23:55.666912Z","iopub.execute_input":"2024-01-14T18:23:55.667579Z","iopub.status.idle":"2024-01-14T18:23:55.698144Z","shell.execute_reply.started":"2024-01-14T18:23:55.667524Z","shell.execute_reply":"2024-01-14T18:23:55.696743Z"},"trusted":true},"execution_count":null,"outputs":[]}]}